{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ulimit -v 80000\n",
    "!ulimit -v 8000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ]))\n",
    "\n",
    "mnist_dataloader = DataLoader(mnist_trainset, batch_size=4)\n",
    "dataloader_size = len(mnist_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADodJREFUeJzt3X9olVUcx/HnqqyscOoKtUZaoYuCKYVWsnS1FWEGlv1AtDUMC8yQqJBiRVLZSgvUtARxNZUMkWU/kIyaC9FG9gvKXFbQ2Ip+WP6cKebtv2/ne/Je7+6e+9zn3u/79df3cLbdkzx9OM+55zlPIplMBgBgSb98DwAAokbwATCH4ANgDsEHwByCD4A5BB8Acwg+AOYQfADMIfgAmDMgyg9LJBI8JhITyWQyke8xFAuu6/jI9LpmxgfAHIIPgDkEHwBzCD4A5hB8AMwh+ACYQ/ABMIfgA2AOwQfAHIIPgDkEHwBzCD4A5hB8AMyJ9HQWAIXhyiuvVO158+ZJXVdXp/qam5ulXr58uer7/PPPczC6vmPGB8Acgg+AOQQfAHMSyWR0h8cWykm1/fv3V+3S0tKMf9ddCznrrLNUX0VFhdQPPPCA6luyZInUM2bMUH1///231I2Njapv4cKFGY/NxQnM4SmU6zqdcePGqfZHH32k2oMGDcro7xw4cEC1y8rK+jawXuIEZgBIgeADYE5Rb2e58MILVbukpETqiRMnqr6qqiqpBw8erPqmT58eyni6urqkXrZsmeq79dZbpT506JDq++qrr6Rua2sLZSzAhAkTpN60aZPq85d33CUx//o8fvy41P6t7dVXXy21v7XF/b2oMeMDYA7BB8Acgg+AOUW3ncX9Wt7/Sr4321LCcPLkSdWePXu21IcPH075e7/88otq//XXX1J3dHSEMja2s4QnzttZ3C1VV1xxhepbt26d1OXl5aovkdCXh5sT/lrdCy+8IPWGDRtS/p2GhgbV99xzz6UdezbYzgIAKRB8AMwpuu0snZ2dUu/bt0/1hXGr297ertr79+9X7euuu05q/+v6tWvX9vnzgd5YtWqV1P4TQdnyb5nPOeccqf3tVtXV1VJXVlaG8vlhYMYHwByCD4A5BB8Ac4puje/PP/+U+tFHH1V9U6dOlfqLL75Qff4jZK4vv/xS6htuuEH1HTlyRLUvv/xyqefPn5/BiIHw+Ccn33zzzVL7W1Rc/trcO++8o9ru6UE///yz6nP/X3K3XgVBEFx//fUZfX7UmPEBMIfgA2BO0T25kY57mKJ/woT7tf+9996r+mbNmiX1G2+8kaPRRYsnN8KT7+s63dNK6Q4Q3bJli9T+VpfJkyertrsVZfXq1arv999/T/kZ//zzj9Q9PT0pPyOslxLx5AYApEDwATCH4ANgTtFtZ0nn4MGDKfv8l6S45syZI/Wbb76p+vwTWIBcGzNmjGq727b8xzL/+OMPqf1Tf15//XWp/dOC3nvvvbTtbAwcOFC1H374YalnzpzZ57/fG8z4AJhD8AEwx9StbjpPPfWU1P7ud/dr99raWtW3devWnI4LCIIgOOOMM6R2n6IIgiCYMmWK1P42rbq6Oql37dql+vxbz6j5LwOLEjM+AOYQfADMIfgAmGPqkbVMXXLJJartPk7jn7jc2tqq2u46yooVK1RflP/Wp8Mja+GJ4rp2X8y9ffv2lD9XU1Oj2vl+Ab37yJp//e/cuVPqa6+9NpTP45E1AEiB4ANgDttZTuGHH35Q7fr6eqmbmppU3913352yffbZZ6u+5uZmqf1d9EA6L730ktT+gZ7u7Wy+b219/fr9N7eK01NOzPgAmEPwATCH4ANgDmt8GWhpaZF67969qs9dewkCvZ1g0aJFqm/kyJFSP/vss6qvu7u7z+NE8XBfjBUE+pRlf1vI22+/HcmYsuGu6/njdl/iFTVmfADMIfgAmEPwATCHNb5e+vrrr1X7zjvvVO1bbrlFan/P3/333y/16NGjVZ//onLY5h8ZVVJSIvVvv/2m+vxTwaPmHpnlHu/m898A99hjj+VqSKfFjA+AOQQfAHO41e0j/7SWtWvXSu2/eHnAgP/+uSdNmqT6qqurpd62bVt4A0TROXbsmGpH/fije2sbBEHQ0NAgtfvioyAIgq6uLqlffPFF1ee/4ChKzPgAmEPwATCH4ANgDmt8vVRZWanat99+u2qPHz9eandNz7d7927V/vjjj0MYHSzIxyNq7iNz/jreXXfdJfXmzZtV3/Tp03M7sCwx4wNgDsEHwBxudU+hoqJCtefNmyf1bbfdpvqGDx+e8d91X7zib0GI0+m0yD//lGW3PW3aNNU3f/780D//oYceUu0nnnhC6tLSUtW3fv16qd0XmMcZMz4A5hB8AMwh+ACYY3aNz1+bmzFjhtTuml4QBMGoUaOy+gz35eJBoE9djvOpucg//7Rit+1fu8uWLZN6zZo1qm/fvn1Suy8lDwL9RsCxY8eqvvLyctXu7OyU+v3331d9K1eu/P9/QMwx4wNgDsEHwJyivtUdNmyYal922WVSv/zyy6rv0ksvzeoz2tvbVXvx4sVS+7vY2bKCMPTv31+1586dK7X/pMTBgwel9g+/TWfHjh2q3draKvWTTz6Z8d+JK2Z8AMwh+ACYQ/ABMCfhf22e0w9LJEL/sKFDh6r2qlWrpHZPlAiCILj44ouz+gx3vcM/Rdb/av/o0aNZfUbUkslk4vQ/hUzk4rr2t5Ns3LhRavcEoFOMRbXT/f/tbnXZsGGD6svFY3BRyPS6ZsYHwByCD4A5BXGre9VVV6m2exDihAkTVN8FF1yQzUcEPT09Urs74YMgCBYtWiT1kSNHsvr7ccOtbnhycavrGzFihNTu+5mDQL/sJ92t7tKlS1XfK6+8IvX3338fyjjzjVtdAEiB4ANgDsEHwJyCWONrbGxUbf9lJ6n4L/R59913pT5x4oTqc7ep+C8JL0as8YUnijU+ZIY1PgBIgeADYE5B3OoifNzqhofrOj641QWAFAg+AOYQfADMIfgAmEPwATCH4ANgDsEHwByCD4A5BB8Acwg+AOZE+sgaAMQBMz4A5hB8AMwh+ACYQ/ABMIfgA2AOwQfAHIIPgDkEHwBzCD4A5hB8AMwh+ACYQ/ABMIfgA2AOwQfAHIIPgDkEHwBzCD4A5hB8AMwh+ACYMyDKD0skErzgIyaSyWQi32MoFlzX8ZHpdc2MD4A5BB8Acwg+AOYQfADMIfgAmEPwATCH4ANgDsEHwByCD4A5BB8Acwg+AOYQfADMIfgAmEPwATCH4ANgDsEHwByCD4A5kZ7AjNRqamqkXr9+veqbPHmy1B0dHZGNCchEQ0OD1AsXLlR9/fr9N7eqrq5WfW1tbTkdVzrM+ACYQ/ABMKcgbnUnTZqk2mVlZVK3tLREPZycGD9+vNSffvppHkcCpFdfX6/aCxYskPrkyZMpfy+ZjM87mZjxATCH4ANgDsEHwJyCWOPzvwYfPXq01IW6xud+zR8EQXDRRRdJPXLkSNWXSPDub8SHf32eeeaZeRpJ9pjxATCH4ANgTkHc6tbV1an2zp078zSS8IwYMUK158yZI/W6detU3549eyIZE5BKbW2t1A8++GDKn/Ov1alTp0r966+/hj+wLDHjA2AOwQfAHIIPgDkFscbnb/0oBqtXr07Zt3fv3ghHAvxfVVWVajc1NUldWlqa8vcWL16s2j/99FO4AwtJ8SUKAJwGwQfAnNje6lZWVko9bNiwPI4kN9LdLnzwwQcRjgT4v3vuuUe1zz///JQ/u23bNqmbm5tzNaRQMeMDYA7BB8Acgg+AObFd45syZYrUAwcOzONIwuOuVbqnsfi6u7ujGA4gzj33XNWePXu2arsnK+/fv1/1PfPMM7kbWI4w4wNgDsEHwJzY3upWVFSk7Pvmm28iHEl4lixZIrW/Ree7776T+tChQ5GNCXaNGjVK6k2bNmX8e8uXL1ft1tbWsIYUGWZ8AMwh+ACYQ/ABMCe2a3zpxOmF24MGDVLtm266SepZs2apvhtvvDHl33n66ael9rcLALngXqvuI6Kn8uGHH0q9dOnSnI0pKsz4AJhD8AEwpyBvdYcOHZrV740dO1Zq/1217stUysvLVV9JSYnUM2fOVH3+IalHjx6Vur29XfUdO3ZM6gED9D/9Z599lnbsQF9NmzZNtRsbG1P+7Pbt21XbPa3lwIED4Q4sD5jxATCH4ANgDsEHwJzYrvG5a2XJZFL1vfrqq1I//vjjGf9N9yt7f43vxIkTUvf09Ki+3bt3S71mzRrVt2vXLtVua2uT2n+BcldXl9T+iTO8NBy5kO1jaT/++KNqx+ll4GFgxgfAHIIPgDkEHwBzYrvGN3fuXKn9lxJPnDgxq7/Z2dkp9VtvvaX6vv32W6k/+eSTrP6+77777lPt8847T2p/DQXIhQULFkjtnqJ8Oun2+BUDZnwAzCH4AJgT21td1/PPP5/vIWSlpqYmZV9vthYAmRo3bpxqpzsRyLV582bV7ujoCG1MccSMD4A5BB8Acwg+AOYUxBpfMWppacn3EFCEtm7dqtpDhgxJ+bPutq36+vpcDSmWmPEBMIfgA2AOt7pAESkrK1PtdE9rrFy5UurDhw/nbExxxIwPgDkEHwBzCD4A5rDGFyH31OcxY8aovrBOhIE9TU1NUvtv/Utnx44duRhOQWDGB8Acgg+AOdzqRsh9aVJvbkkAl38CS21trdT+9pXjx49LvWLFCtVXbC8Q6g3+7wNgDsEHwByCD4A5rPHlyTXXXKPar732Wn4GgoIzePBg1R4+fHjKn+3u7pb6kUceydmYCg0zPgDmEHwAzOFWN0LukxsA8ocZHwBzCD4A5hB8AMxhjS+HtmzZotp33HFHnkaCYrJnzx7Vdk9Zqaqqino4BYkZHwBzCD4A5iTcE0Ny/mGJRHQfhrSSySR7a0LCdR0fmV7XzPgAmEPwATCH4ANgDsEHwByCD4A5BB8Acwg+AOYQfADMIfgAmEPwATAn0kfWACAOmPEBMIfgA2AOwQfAHIIPgDkEHwBzCD4A5hB8AMwh+ACYQ/ABMIfgA2AOwQfAHIIPgDkEHwBzCD4A5hB8AMwh+ACYQ/ABMIfgA2AOwQfAHIIPgDkEHwBzCD4A5hB8AMz5F4/ufYQUFrBkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, _ = next(iter(mnist_dataloader))\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(image[0, :, :], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, image_size=128, n_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.init_size = np.ceil(image_size / 8).astype(int)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, 128 * self.init_size * self.init_size)\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64)            \n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=(5, 5), stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, n_channels, kernel_size=(5, 5), stride=2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, z):\n",
    "        hidden = self.fc1(z)\n",
    "        hidden = hidden.view(-1, 128, self.init_size, self.init_size)\n",
    "        hidden = self.conv_block1(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        \n",
    "        hidden = self.conv_block2(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        \n",
    "        hidden = self.conv_block3(hidden)\n",
    "        output = self.tanh(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size, n_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.fc_size = np.ceil(self.image_size / 8).astype(int)\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 32, kernel_size=(5, 5), stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(32)            \n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(5, 5), stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * self.fc_size * self.fc_size, 256),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv_block2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv_block3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.view(-1, 128 * self.fc_size * self.fc_size)\n",
    "        \n",
    "        output = self.fc(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "generator_lr = 2e-6\n",
    "discriminator_lr = 1e-4\n",
    "\n",
    "generator = Generator(100, 28, n_channels=1)\n",
    "discriminator = Discriminator(28, n_channels=1)\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=generator_lr)\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=discriminator_lr)\n",
    "\n",
    "adversarial_loss = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-9152237788e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mrunning_d_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mdataloader_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mrunning_g_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mdataloader_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_size' is not defined"
     ]
    }
   ],
   "source": [
    "d_history = []\n",
    "g_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    running_d_loss = 0\n",
    "    running_g_loss = 0\n",
    "    for images, _ in mnist_dataloader:\n",
    "        real = Variable(torch.ones(images.shape[0], 1), requires_grad=False)\n",
    "        fake = Variable(torch.zeros(images.shape[0], 1), requires_grad=False)\n",
    "\n",
    "        z = Variable(torch.Tensor(np.random.randn(images.shape[0], 100)))\n",
    "        generated_images = generator(z)\n",
    "\n",
    "        g_loss = adversarial_loss(discriminator(generated_images), real)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(images), real)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_images.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        running_d_loss += d_loss\n",
    "        running_g_loss += g_loss\n",
    "\n",
    "        \n",
    "    running_d_loss /= dataloader_size\n",
    "    running_g_loss /= dataloader_size\n",
    "    \n",
    "    d_history.append(running_d_loss)\n",
    "    g_history.append(running_g_loss)\n",
    "    \n",
    "    real_img = images[0].permute(1, 2, 0).data.numpy()\n",
    "    gen_img = generated_images[0].permute(1,2,0).data.numpy()\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('gen_img: {:.4f}, {:.4f}'.format(gen_img.mean(), gen_img.var()))\n",
    "    plt.imshow(gen_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('real_img: {:.4f}, {:.4f}'.format(real_img.mean(), real_img.var()))\n",
    "    plt.imshow(real_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print (\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, n_epochs,\n",
    "                                                    running_d_loss, running_g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "  running_d_loss /= dataloader_size\n",
    "    running_g_loss /= dataloader_size\n",
    "    \n",
    "    d_history.append(running_d_loss)\n",
    "    g_history.append(running_g_loss)\n",
    "    \n",
    "    real_img = images[0].permute(1, 2, 0).data.numpy()\n",
    "    gen_img = generated_images[0].permute(1,2,0).data.numpy()\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('gen_img: {:.4f}, {:.4f}'.format(gen_img.mean(), gen_img.var()))\n",
    "    plt.imshow(gen_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('real_img: {:.4f}, {:.4f}'.format(real_img.mean(), real_img.var()))\n",
    "    plt.imshow(real_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print (\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, n_epochs,\n",
    "                                                    running_d_loss, running_g_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowflake]",
   "language": "python",
   "name": "conda-env-snowflake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
